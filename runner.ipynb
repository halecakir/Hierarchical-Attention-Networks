{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "\n",
    "import common\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMISSION_TYPE=\"READ_CALENDAR\"\n",
    "MODEL_TYPE=\"HAN\"\n",
    "OUTPUT_DIR=\"output/reports/{}\".format(MODEL_TYPE)\n",
    "PARAMETERS_DIR=\"/home/huseyinalecakir/HAN/data/saved-parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    permission_type = PERMISSION_TYPE\n",
    "    useful_reviews = 5\n",
    "    saved_data = \"{}/saved-data/emdeddings-documents-w2i.pickle\".format(PARAMETERS_DIR)\n",
    "    saved_reviews = \"{}/saved-data/reviews.pickle\".format(PARAMETERS_DIR)\n",
    "    saved_predicted_reviews = \"{}/saved-data/predicted-{}-reviews.pickle\".format(PARAMETERS_DIR, PERMISSION_TYPE)\n",
    "    model_checkpoint = \"{}/saved-models/{}-{}.pt\".format(PARAMETERS_DIR, MODEL_TYPE, PERMISSION_TYPE)\n",
    "    outdir = \"{}/{}-{}.out\".format(OUTPUT_DIR, MODEL_TYPE, PERMISSION_TYPE)\n",
    "    stemmer = \"porter\"\n",
    "    disable_cuda = False\n",
    "    hidden_size = 128\n",
    "    init_weight = 0.08\n",
    "    output_size = 1\n",
    "    grad_clip = 5\n",
    "    dropout = 0\n",
    "    dropoutrec = 0\n",
    "    learning_rate_decay = 0.985\n",
    "    learning_rate_decay_after = 2\n",
    "    print_every = 100\n",
    "    device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Opts()\n",
    "\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.w2i = None\n",
    "        self.entries = None\n",
    "        self.train_entries = None\n",
    "        self.test_entries = None\n",
    "        self.ext_embedding = None\n",
    "        self.reviews = None\n",
    "        self.predicted_reviews = None\n",
    "\n",
    "    def to(self, device):\n",
    "        if self.entries:\n",
    "            for document in self.entries:\n",
    "                for i in range(len(document.index_tensors)):\n",
    "                    document.index_tensors[i] = document.index_tensors[i].to(\n",
    "                        device=device\n",
    "                    )\n",
    "        if self.reviews:\n",
    "            for doc_id in self.reviews:\n",
    "                for review in self.reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "        if self.predicted_reviews:\n",
    "            for doc_id in self.predicted_reviews:\n",
    "                for review in self.predicted_reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "\n",
    "    def load(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.load(target)\n",
    "\n",
    "    def save_data(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.dump(target)\n",
    "\n",
    "    def load_predicted_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.predicted_reviews = pickle.load(target)\n",
    "        for app_id in self.predicted_reviews.keys():\n",
    "            self.predicted_reviews[app_id].sort(\n",
    "                key=lambda x: x.prediction_result.item(), reverse=True\n",
    "            )\n",
    "\n",
    "    def load_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.reviews = pickle.load(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, w2i):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.w2i = w2i\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.opt.hidden_size, self.opt.hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.embedding = nn.Embedding(len(self.w2i), self.opt.hidden_size)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def initalizedPretrainedEmbeddings(self, embeddings):\n",
    "        weights_matrix = np.zeros(((len(self.w2i), self.opt.hidden_size)))\n",
    "        for word in self.w2i:\n",
    "            weights_matrix[self.w2i[word]] = embeddings[word]\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(weights_matrix))\n",
    "\n",
    "    def forward(self, input_src):\n",
    "        src_emb = self.embedding(input_src)  # batch_size x src_length x emb_size\n",
    "        outputs, (h, c) = self.gru(src_emb)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = 2*opt.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, opt.output_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def forward(self, prev_h):\n",
    "        if self.opt.dropout > 0:\n",
    "            prev_h = self.dropout(prev_h)\n",
    "        h2y = self.linear(prev_h)\n",
    "        pred = self.sigmoid(h2y)\n",
    "        return pred\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Attention, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.hidden_size\n",
    "        self.linear = nn.Linear(2*self.hidden_size, opt.hidden_size)\n",
    "        self.context = torch.rand(self.hidden_size, device=self.opt.device, requires_grad=True)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(inputs.shape[1], -1)\n",
    "        hidden_repr = self.linear(inputs)\n",
    "        hidden_with_context = torch.exp(torch.mv(hidden_repr, self.context))\n",
    "        normalized_weight = hidden_with_context/torch.sum(hidden_with_context)\n",
    "        normalized_hidden_repr = torch.sum(inputs.t()*normalized_weight, dim=1)\n",
    "        return normalized_hidden_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.opt = None\n",
    "        self.encoders = {}\n",
    "        self.attentions = {}\n",
    "        self.review_encoder = None\n",
    "        self.classifier = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "\n",
    "    def create(self, opt, data):\n",
    "        self.opt = opt\n",
    "        self.encoders[\"sentenceL1\"] = Encoder(self.opt, data.w2i).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "        self.attentions[\"word_attention\"] = Attention(self.opt).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "        self.encoders[\"sentenceL2\"] = nn.GRU(2*self.opt.hidden_size, self.opt.hidden_size, bidirectional=True, batch_first=True).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "        self.attentions[\"sentence_attention\"] = Attention(self.opt).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "    \n",
    "        params = []\n",
    "        for encoder in self.encoders:\n",
    "            params += list(self.encoders[encoder].parameters())\n",
    "        self.classifier = Classifier(self.opt).to(device=self.opt.device)\n",
    "        params += list(self.classifier.parameters())\n",
    "        self.optimizer = optim.RMSprop(params)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def train(self):\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].train()\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].train()\n",
    "        self.classifier.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].eval()\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate_decay(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] * self.opt.learning_rate_decay\n",
    "\n",
    "    def grad_clip(self):\n",
    "        for encoder in self.encoders:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                self.encoders[encoder].parameters(), self.opt.grad_clip\n",
    "            )\n",
    "        for attention in self.attentions:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                self.attentions[attention].parameters(), self.opt.grad_clip\n",
    "            )\n",
    "        torch.nn.utils.clip_grad_value_(\n",
    "            self.classifier.parameters(), self.opt.grad_clip\n",
    "        )\n",
    "\n",
    "    def save(self, filename):\n",
    "        checkpoint = {}\n",
    "        checkpoint[\"opt\"] = self.opt\n",
    "        for encoder in self.encoders:\n",
    "            checkpoint[encoder] = self.encoders[encoder].state_dict()\n",
    "        for attention in self.attentions:\n",
    "            checkpoint[attention] = self.attentions[attention].state_dict()\n",
    "        checkpoint[\"classifier\"] = self.classifier.state_dict()\n",
    "        checkpoint[\"optimizer\"] = self.optimizer.state_dict()\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load(self, filename, data):\n",
    "        checkpoint = torch.load(filename)\n",
    "        opt = checkpoint[\"opt\"]\n",
    "        self.create(opt, data)\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].load_state_dict(checkpoint[encoder])\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].load_state_dict(checkpoint[attention])\n",
    "        self.classifier.load_state_dict(checkpoint[\"classifier\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\n",
    "def write_file(filename, string):\n",
    "    with open(filename, \"a\") as target:\n",
    "        target.write(\"{}\\n\".format(string))\n",
    "        target.flush()\n",
    "\n",
    "\n",
    "def train_item(args, model, document):\n",
    "    model.zero_grad()\n",
    "    h_n = None, None\n",
    "    first_word = True\n",
    "    sentence_contexts = []\n",
    "    for sentence_index_tensor in document.index_tensors:\n",
    "        if sentence_index_tensor.shape[1] > 0:\n",
    "            outputs, hidden = model.encoders[\"sentenceL1\"](sentence_index_tensor)\n",
    "            context = model.attentions[\"word_attention\"](outputs)\n",
    "            sentence_contexts.append(context)\n",
    "    contexts = torch.zeros([1, len(sentence_contexts), 256], device=args.device)\n",
    "    for idx, c in enumerate(sentence_contexts):\n",
    "        contexts[0,idx,:] = c\n",
    "        \n",
    "    outputs, hidden = model.encoders[\"sentenceL2\"](contexts)\n",
    "    context = model.attentions[\"sentence_attention\"](outputs)\n",
    "    pred = model.classifier(context)\n",
    "    loss = model.criterion(\n",
    "        pred,\n",
    "        torch.tensor(\n",
    "            [[[document.permissions[args.permission_type]]]], dtype=torch.float\n",
    "        ).to(device=args.device),\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    if model.opt.grad_clip != -1:\n",
    "        model.grad_clip()\n",
    "    model.step()\n",
    "    return loss\n",
    "\n",
    "def test_item(model, document):\n",
    "    \n",
    "    h_n = None, None\n",
    "    first_word = True\n",
    "    sentence_contexts = []\n",
    "    for sentence_index_tensor in document.index_tensors:\n",
    "        if sentence_index_tensor.shape[1] > 0:\n",
    "            outputs, hidden = model.encoders[\"sentenceL1\"](sentence_index_tensor)\n",
    "            context = model.attentions[\"word_attention\"](outputs)\n",
    "            sentence_contexts.append(context)\n",
    "    contexts = torch.zeros([1, len(sentence_contexts), 256], device=args.device)\n",
    "    for idx, c in enumerate(sentence_contexts):\n",
    "        contexts[0,idx,:] = c\n",
    "        \n",
    "    outputs, hidden = model.encoders[\"sentenceL2\"](contexts)\n",
    "    context = model.attentions[\"sentence_attention\"](outputs)\n",
    "    pred = model.classifier(context)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_all(args, model, data):\n",
    "    write_file(args.outdir, \"Training...\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for index, document in enumerate(data.train_entries):\n",
    "        loss = train_item(args, model, document)\n",
    "        if index != 0:\n",
    "            if index % model.opt.print_every == 0:\n",
    "                write_file(\n",
    "                    args.outdir,\n",
    "                    \"Index {} Loss {}\".format(\n",
    "                        index, np.mean(losses[index - model.opt.print_every :])\n",
    "                    ),\n",
    "                )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "def test_all(args, model, data):\n",
    "    def pr_roc_auc(predictions, gold):\n",
    "        y_true = np.array(gold)\n",
    "        y_scores = np.array(predictions)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "        pr_auc = average_precision_score(y_true, y_scores)\n",
    "        return roc_auc, pr_auc\n",
    "\n",
    "    write_file(args.outdir, \"Predicting..\")\n",
    "\n",
    "    predictions, gold = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, document in enumerate(data.test_entries):\n",
    "            pred = test_item(model, document)\n",
    "            predictions.append(pred.cpu())\n",
    "            gold.append(document.permissions[args.permission_type])\n",
    "    return pr_roc_auc(predictions, gold)\n",
    "\n",
    "\n",
    "def kfold_validation(args, data):\n",
    "    data.entries = np.array(data.entries)\n",
    "    random.shuffle(data.entries)\n",
    "\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    roc_l, pr_l = [], []\n",
    "    for foldid, (train, test) in enumerate(kfold.split(data.entries)):\n",
    "        write_file(args.outdir, \"Fold {}\".format(foldid + 1))\n",
    "\n",
    "        model = Model()\n",
    "        model.create(args, data)\n",
    "        data.train_entries = data.entries[train]\n",
    "        data.test_entries = data.entries[test]\n",
    "\n",
    "        train_all(args, model, data)\n",
    "        roc_auc, pr_auc = test_all(args, model, data)\n",
    "\n",
    "        write_file(args.outdir, \"ROC {} PR {}\".format(roc_auc, pr_auc))\n",
    "        roc_l.append(roc_auc)\n",
    "        pr_l.append(pr_auc)\n",
    "    write_file(args.outdir, \"Summary : ROC {} PR {}\".format(np.mean(roc_l), np.mean(pr_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.load(args.saved_data)\n",
    "data.to(args.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n",
      "/home/huseyinalecakir/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "kfold_validation(args, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:,0] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
