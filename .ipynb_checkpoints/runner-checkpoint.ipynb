{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "\n",
    "import common\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMISSION_TYPE=\"READ_CALENDAR\"\n",
    "MODEL_TYPE=\"HAN\"\n",
    "OUTPUT_DIR=\"output/reports/{}\".format(MODEL_TYPE)\n",
    "PARAMETERS_DIR=\"/home/huseyinalecakir/HAN/data/saved-parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    permission_type = PERMISSION_TYPE\n",
    "    useful_reviews = 5\n",
    "    saved_data = \"{}/saved-data/emdeddings-documents-w2i.pickle\".format(PARAMETERS_DIR)\n",
    "    saved_reviews = \"{}/saved-data/reviews.pickle\".format(PARAMETERS_DIR)\n",
    "    saved_predicted_reviews = \"{}/saved-data/predicted-{}-reviews.pickle\".format(PARAMETERS_DIR, PERMISSION_TYPE)\n",
    "    model_checkpoint = \"{}/saved-models/{}-{}.pt\".format(PARAMETERS_DIR, MODEL_TYPE, PERMISSION_TYPE)\n",
    "    outdir = \"{}/{}-{}.out\".format(OUTPUT_DIR, MODEL_TYPE, PERMISSION_TYPE)\n",
    "    stemmer = \"porter\"\n",
    "    disable_cuda = False\n",
    "    hidden_size = 128\n",
    "    init_weight = 0.08\n",
    "    output_size = 1\n",
    "    grad_clip = 5\n",
    "    dropout = 0\n",
    "    dropoutrec = 0\n",
    "    learning_rate_decay = 0.985\n",
    "    learning_rate_decay_after = 2\n",
    "    print_every = 100\n",
    "    device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Opts()\n",
    "\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.w2i = None\n",
    "        self.entries = None\n",
    "        self.train_entries = None\n",
    "        self.test_entries = None\n",
    "        self.ext_embedding = None\n",
    "        self.reviews = None\n",
    "        self.predicted_reviews = None\n",
    "\n",
    "    def to(self, device):\n",
    "        if self.entries:\n",
    "            for document in self.entries:\n",
    "                for i in range(len(document.index_tensors)):\n",
    "                    document.index_tensors[i] = document.index_tensors[i].to(\n",
    "                        device=device\n",
    "                    )\n",
    "        if self.reviews:\n",
    "            for doc_id in self.reviews:\n",
    "                for review in self.reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "        if self.predicted_reviews:\n",
    "            for doc_id in self.predicted_reviews:\n",
    "                for review in self.predicted_reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "\n",
    "    def load(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.load(target)\n",
    "\n",
    "    def save_data(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.dump(target)\n",
    "\n",
    "    def load_predicted_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.predicted_reviews = pickle.load(target)\n",
    "        for app_id in self.predicted_reviews.keys():\n",
    "            self.predicted_reviews[app_id].sort(\n",
    "                key=lambda x: x.prediction_result.item(), reverse=True\n",
    "            )\n",
    "\n",
    "    def load_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.reviews = pickle.load(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, w2i):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.w2i = w2i\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.opt.hidden_size, self.opt.hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.embedding = nn.Embedding(len(self.w2i), self.opt.hidden_size)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def initalizedPretrainedEmbeddings(self, embeddings):\n",
    "        weights_matrix = np.zeros(((len(self.w2i), self.opt.hidden_size)))\n",
    "        for word in self.w2i:\n",
    "            weights_matrix[self.w2i[word]] = embeddings[word]\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(weights_matrix))\n",
    "\n",
    "    def forward(self, input_src):\n",
    "        src_emb = self.embedding(input_src)  # batch_size x src_length x emb_size\n",
    "        outputs, (h, c) = self.gru(src_emb)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, opt.output_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def forward(self, prev_h):\n",
    "        if self.opt.dropout > 0:\n",
    "            prev_h = self.dropout(prev_h)\n",
    "        h2y = self.linear(prev_h)\n",
    "        pred = self.sigmoid(h2y)\n",
    "        return pred\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Attention, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.hidden_size\n",
    "        self.linear = nn.Linear(2*self.hidden_size, opt.hidden_size)\n",
    "        self.context = torch.rand(self.hidden_size, device=self.opt.device, requires_grad=True)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -self.opt.init_weight, self.opt.init_weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(inputs.shape[1], -1)\n",
    "        hidden_repr = self.linear(inputs)\n",
    "        hidden_with_context = torch.exp(torch.mv(hidden_repr, self.context))\n",
    "        normalized_weight = hidden_with_context/torch.sum(hidden_with_context)\n",
    "        normalized_hidden_repr = torch.sum(inputs.t()*normalized_weight, dim=1)\n",
    "        return normalized_hidden_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.opt = None\n",
    "        self.encoders = {}\n",
    "        self.attentions = {}\n",
    "        self.review_encoder = None\n",
    "        self.classifier = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "\n",
    "    def create(self, opt, data):\n",
    "        self.opt = opt\n",
    "        self.encoders[\"sentenceL1\"] = Encoder(self.opt, data.w2i).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "        self.attentions[\"word_attention\"] = Attention(self.opt).to(\n",
    "            device=self.opt.device\n",
    "        )\n",
    "        self.encoders[\"sentenceL2\"] = nn.GRUCell(\n",
    "            2*self.opt.hidden_size, self.opt.hidden_size)\n",
    "    \n",
    "        params = []\n",
    "        for encoder in self.encoders:\n",
    "            params += list(self.encoders[encoder].parameters())\n",
    "        self.classifier = Classifier(self.opt).to(device=self.opt.device)\n",
    "        params += list(self.classifier.parameters())\n",
    "        self.optimizer = optim.RMSprop(params)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def train(self):\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].train()\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].train()\n",
    "        self.classifier.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].eval()\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].eval()\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate_decay(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = param_group[\"lr\"] * self.opt.learning_rate_decay\n",
    "\n",
    "    def grad_clip(self):\n",
    "        for encoder in self.encoders:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                self.encoders[encoder].parameters(), self.opt.grad_clip\n",
    "            )\n",
    "        for attention in self.attentions:\n",
    "            torch.nn.utils.clip_grad_value_(\n",
    "                self.attentions[attention].parameters(), self.opt.grad_clip\n",
    "            )\n",
    "        torch.nn.utils.clip_grad_value_(\n",
    "            self.classifier.parameters(), self.opt.grad_clip\n",
    "        )\n",
    "\n",
    "    def save(self, filename):\n",
    "        checkpoint = {}\n",
    "        checkpoint[\"opt\"] = self.opt\n",
    "        for encoder in self.encoders:\n",
    "            checkpoint[encoder] = self.encoders[encoder].state_dict()\n",
    "        for attention in self.attentions:\n",
    "            checkpoint[attention] = self.attentions[attention].state_dict()\n",
    "        checkpoint[\"classifier\"] = self.classifier.state_dict()\n",
    "        checkpoint[\"optimizer\"] = self.optimizer.state_dict()\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load(self, filename, data):\n",
    "        checkpoint = torch.load(filename)\n",
    "        opt = checkpoint[\"opt\"]\n",
    "        self.create(opt, data)\n",
    "        for encoder in self.encoders:\n",
    "            self.encoders[encoder].load_state_dict(checkpoint[encoder])\n",
    "        for attention in self.attentions:\n",
    "            self.attentions[attention].load_state_dict(checkpoint[attention])\n",
    "        self.classifier.load_state_dict(checkpoint[\"classifier\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\n",
    "def write_file(filename, string):\n",
    "    with open(filename, \"a\") as target:\n",
    "        target.write(\"{}\\n\".format(string))\n",
    "        target.flush()\n",
    "\n",
    "\n",
    "def train_item(args, model, document):\n",
    "    model.zero_grad()\n",
    "    sentence_encodings = []\n",
    "    for sentence_index_tensor in document.index_tensors:\n",
    "        if sentence_index_tensor.shape[1] > 0:\n",
    "            outputs_s, (hidden_s, cell_s) = model.encoders[\"sentenceL1\"](sentence_index_tensor)\n",
    "            context = model.attentions[\"word_attention\"](outputs_s)\n",
    "            sentence_encodings.append(context)\n",
    "    torch.zeros([len(sentence_encodings), sentence_encodings[0].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.load(args.saved_data)\n",
    "data.to(args.device)\n",
    "\n",
    "data.entries = np.array(data.entries)\n",
    "random.shuffle(data.entries)\n",
    "data.test_entries = data.entries[: int(len(data.entries) / 10)]\n",
    "data.train_entries = data.entries[int(len(data.entries) / 10) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.create(args, data)\n",
    "document = data.train_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 256])\n",
      "torch.Size([1, 7, 256])\n",
      "torch.Size([1, 7, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 18, 256])\n",
      "torch.Size([1, 19, 256])\n",
      "torch.Size([1, 8, 256])\n",
      "torch.Size([1, 13, 256])\n",
      "torch.Size([1, 4, 256])\n",
      "torch.Size([1, 9, 256])\n",
      "torch.Size([1, 6, 256])\n",
      "torch.Size([1, 13, 256])\n",
      "torch.Size([1, 15, 256])\n",
      "torch.Size([1, 9, 256])\n",
      "torch.Size([1, 10, 256])\n",
      "torch.Size([1, 6, 256])\n",
      "torch.Size([1, 10, 256])\n",
      "torch.Size([1, 9, 256])\n",
      "torch.Size([1, 9, 256])\n",
      "torch.Size([1, 14, 256])\n"
     ]
    }
   ],
   "source": [
    "train_item(args, model, document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
